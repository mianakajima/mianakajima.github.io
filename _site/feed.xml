<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-07-30T22:27:44-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Learnings</title><author><name>Mia Nakajima</name><email>mb.nakajima@gmail.com</email></author><entry><title type="html">Stochastic Optimization of Electric Grid Forecasts</title><link href="http://localhost:4000/2022/07/22/QuadraticProgramming/" rel="alternate" type="text/html" title="Stochastic Optimization of Electric Grid Forecasts" /><published>2022-07-22T00:00:00-07:00</published><updated>2022-07-22T00:00:00-07:00</updated><id>http://localhost:4000/2022/07/22/QuadraticProgramming</id><content type="html" xml:base="http://localhost:4000/2022/07/22/QuadraticProgramming/"><![CDATA[<p>I am currently trying to reproduce a paper “Task-based End-to-end Model Learning in Stochastic Optimization” by Priya Donti, Brandon Amos and J. Zico Kolter.
In this paper, they propose a method to learn to predict something, taking into account the end task that the prediction would be used for.</p>

<p>This wouldn’t be necessary if we could predict what we’re trying to predict perfectly, but we know that’s not always the case ;) Since our model won’t be perfect, we want to make sure that the model performs well in the task it will be used for.</p>

<p>For example, the authors ran an experiment using this method on electric grid scheduling. I summarize how the electric grid scheduling is broken down using this method in the table below. The “Experiment” column shows the prediction and task associated with electric grid scheduling, while the “Method” column shows how we would find the solution to each experiment part.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Experiment</th>
      <th>Method</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prediction</td>
      <td>Electric load</td>
      <td>Neural network/Probabilistic Model</td>
    </tr>
    <tr>
      <td>Task</td>
      <td>Minimize the cost of a certain generator schedule (ie., both overgenerating and underegenerating electricity can incur extra costs when operating the grid)</td>
      <td>Stochastic Optimization</td>
    </tr>
  </tbody>
</table>

<p>In practice, we would first generate a neural network model to predict the electric demand/how much electricity we should generate. Then, we would find tune that model using stochastic optimization, to minize the cost on the grid.</p>

<p>Today, I want to walk through the stochastic optimization part of this method since it was not straight forward and think it’s pretty cool :)</p>

<h3 id="what-is-stochastic-optimization">What is Stochastic Optimization?</h3>

<p>In optimization, we are trying to find the optimal solution to something - usually that means we are finding the minimum or maximum of a function. Stochastic optimization is not that different. Stochastic implies a component of randomness, and stochastic optimization is just that: <strong>optimization of a function which includes random variables or constraints</strong>. The randomness may be able to be described as coming from a known distribution, or sometimes it may be unknown.</p>

<p>In this paper’s case, the function we are trying to minimize is the cost of using a certain generator schedule. In grid scheduling, usually how much energy to generate is decided for the whole 24 hours of the next day and then adjustments are made the day of as needed. We assume there are costs associated with both undergenerating and overgenerating.</p>

<p>The paper assumes the cost function looks like this:</p>

\[\min_{z \in \mathbb{R}^{24}} \sum_{i = 1}^{24} \mathbb{E}_{y~p(y\vert x;\theta)} [\gamma_s[y_i -z_i]_+ + \gamma_e[z_i - y_i]_+ + \frac{1}{2}(z_i - y_i)^2]\]

\[s.t. \vert z_i - z_{i - 1} \vert \leq c_r \forall i\]

<p>Where:</p>
<ul>
  <li>$z_i$ : Electricity generation scheduled for every hour $i$</li>
  <li>$y_i$ : Demand, which we assume comes from a Gaussian distribution</li>
  <li>$\gamma_s$: The cost of undergenerating</li>
  <li>$\gamma_e$: The cost of overgenerating</li>
  <li>$c_r$: Ramping constant (we assume the energy generation cannot change excessively from one hour to the next)</li>
  <li>$[x]_+$ : Function meaning take the maximum of $x$ or 0.</li>
</ul>

<p>Since we don’t know actually how much demand for energy there will be and we must assume it’s a random variable, this is a stochastic optimization problem.</p>

<p>The authors of this paper solve this problem using a “stochastic gradient approach”. I understood this to mean stochastic gradient descent using the cost function above rather than standard cost functions, such as root mean square error. The specific steps are:</p>

<ol>
  <li>Say $x$ (features such as weather) and $y$ (the true demand) come from some true distribution, $D$. And we are able to create an initial model $z = f(x \vert \theta)$ that predicts the demand, $z$ from features $x$.</li>
  <li>Sample $(x, y)$ from true distrubution $D$</li>
  <li>Solve for $z$ that will minimize the above cost function using sample.</li>
  <li>Now do gradient descent. Update the parameters $\theta$ by taking derivates of the cost function.</li>
  <li>Repeat steps 2-4 for a chosen number of epochs.</li>
</ol>

<p>Now the question arises: how do we solve for the best solution of $z$? For this, we can use an approach called <a href="https://en.wikipedia.org/wiki/Sequential_quadratic_programming">sequential quadratic programming</a>.</p>

<h3 id="sequential-quadratic-programming">Sequential Quadratic Programming</h3>

<p>The solution to minimizing the cost function does not look straightforward! Here, we can try using sequential quadratic programming. The big picture is that we are using local quadratic approximations of our function in order to iteratively shimmy our estimate to the real minimum of the function.</p>

<p>We can approximate the function at our intial guess of $z^{(j)}$ using a Taylor approximation to the second degree. It looks like this:</p>

\[Define: f(z) =  \mathbb{E}_{y~p(y\vert x;\theta)} [\gamma_s[y_i -z_i]_+ + \gamma_e[z_i - y_i]_+ + \frac{1}{2}(z_i - y_i)^2]\]

\[d = z^{(j + 1)} - z^{(j)}\]

\[f \approx  \nabla f(z^{(j)})d + \frac{1}{2} d^T\nabla^2 f(z^{(j)})d\]

<p>We can now actually change our problem from finding $z$ which minimizes $f(z)$ to finding $d$ which minimizes the approximation of $f(z)$  as shown below.</p>

\[d = arg \min_{d} \frac{1}{2} d^T\nabla^2 f(z^{(j)})d + \nabla f(z^{(j)})d\]

<p>We can see this follows the form of a quadratic program which has a general form:</p>

\[arg \min_{x}  \frac{1}{2} x^TQx + P^Tx\]

<p>Where:</p>

<ul>
  <li>$Q$ is a nxn matrix and $P$ is a n-dimensional vector</li>
</ul>

<p>We want to do this because quadratic programs may be easier to solve than our initial function. Dependent on the characteristics of matrix $Q$, there are known ways to solve for the minimum of the equation. Some examples are <a href="https://en.wikipedia.org/wiki/Quadratic_programming">here</a>. This paper uses a Python package <code class="language-plaintext highlighter-rouge">qpth</code> to solve the quadratic program.</p>

<p>After solving for $d$, we can now find our new estimate $z^{(j + 1)} = z^{(j)} + d$.</p>

<p>We can iteratively solve for $z^{(j + 1)}$ until we converge, ie., until $\vert z^{(j + 1)} - z^{(j)} \vert &lt; \delta$ for some small $\delta$.</p>

<h2 id="tying-it-all-together">Tying It All Together</h2>

<p>Whew! So that was a lot of math and lots of talk on iterations.</p>

<p>This process can be summed up as follows:</p>

<ol>
  <li>In a scenario where you have a problem where you may want to assess task-based accuracy (which is a stochastic programming problem) in addition to predictive accuracy, one solution may be to train a probablistic predictive model and fine tune it based on a cost function related to your task.</li>
  <li>The fine tuning involves stochastic gradient desent of your inital model using the task-based cost function.</li>
  <li>In the electric grid scheduling case, minimizng the cost function was complex so we had to use sequential quadratic programming.</li>
</ol>

<p>And that’s it for this week! :)</p>]]></content><author><name>Mia Nakajima</name><email>mb.nakajima@gmail.com</email></author><summary type="html"><![CDATA[I am currently trying to reproduce a paper “Task-based End-to-end Model Learning in Stochastic Optimization” by Priya Donti, Brandon Amos and J. Zico Kolter. In this paper, they propose a method to learn to predict something, taking into account the end task that the prediction would be used for.]]></summary></entry><entry><title type="html">Learnings from Kaggle House Prices Competition - Part 1</title><link href="http://localhost:4000/2022/07/13/MLLearnings/" rel="alternate" type="text/html" title="Learnings from Kaggle House Prices Competition - Part 1" /><published>2022-07-13T00:00:00-07:00</published><updated>2022-07-13T00:00:00-07:00</updated><id>http://localhost:4000/2022/07/13/MLLearnings</id><content type="html" xml:base="http://localhost:4000/2022/07/13/MLLearnings/"><![CDATA[<p>I wanted to try getting familiar with sklearn’s machine learning pipeline, so I started applying it to the Kaggle House Prices Competition. The data comprised of housing prices in Ames, Iowa with 79 features. I ended up learning a lot through the whole process.</p>

<h2 id="data-exploration-and-cleaning">Data Exploration and Cleaning</h2>

<p>This was a great dataset (probably purposefully so, being a “Getting Started” competition) to practice a workflow for analyzing data since there were so many features. It definitely reminded me that I need to have the patience to observe all the data. I ended up going through a couple rounds of data cleaning.</p>

<p>To make the data cleaning workflow more streamlined in the future, I came up with this workflow:</p>

<p><strong>Steps for reviewing data</strong>:</p>

<ul>
  <li>Review data explanations if available!</li>
  <li>Review missing data
    <ul>
      <li><em>Purpose</em>: To omit columns that have too many missing values, identify data cleaning approach (ie., imputation or other)</li>
      <li>Make sure to understand why data is missing. For example, in this dataset the data is NA if the house does not have a certain feature (ie., a fence). This is valuable information and should be cleaned to represent that, and not be tossed out.</li>
    </ul>
  </li>
  <li>Visualization
    <ul>
      <li><em>Purpose</em>: This can help understand correlations between the features and dependent variable. Additionally, this can help show any quirks of the dataset.
        <ul>
          <li>Look at distribution of output variable - is it normally distributed? If not, consider transforming the variable.</li>
          <li>Make pair scatter plots of all the numeric variables.</li>
          <li>Make boxplots of categorical variables with respect to the dependent variable</li>
          <li>Any others that might be helpful</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="the-sklearn-pipeline">The sklearn pipeline</h2>

<h3 id="first-some-terminology">First, some terminology</h3>

<p>One thing I learned is that there are many words in machine learning that sound scary but are actually very simple concepts. For example, “pipeline” sounds very scary, but it’s just a fancy word to describe a way to apply a series of functions to your data, either to clean your data, transform it or model it.</p>

<p>Below are some of such words that I’ll keep be using throughout:</p>

<table>
  <thead>
    <tr>
      <th>Terminology</th>
      <th>In “English”</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Pipeline</td>
      <td>An object you can use to apply many functions to your data (for data cleaning, transforming or modeling)</td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>A function to “transform” your data, which can be as simple as scaling it</td>
    </tr>
    <tr>
      <td>Custom Transformer</td>
      <td>A function you write that can be applied to your pipeline</td>
    </tr>
    <tr>
      <td>Feature Engineering</td>
      <td>You use the variables (features) you have to come up with new ones that may be helpful. For example if you have square footage and rooms, maybe you want to know the square footage per room.</td>
    </tr>
  </tbody>
</table>

<h3 id="the-pipeline">The Pipeline</h3>
<p>Using sklearn’s pipeline was super streamlined! Below shows a code snippet of what the pipeline looks like. The <code class="language-plaintext highlighter-rouge">ColumnTransfomer()</code> function allows you to apply different pipelines to the data by the column name. In this case, I had 3 separate pipelines by the data type. Each pipeline is composed of sklearn functions or custom transformers.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Pipeline for numeric variables
</span><span class="n">numeric_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">'postprocess'</span><span class="p">,</span> <span class="n">CleanRemodels</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">'impute'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span> <span class="o">=</span> <span class="s">'median'</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">'homearea'</span><span class="p">,</span> <span class="n">add_home_area</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">'scale'</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Pipeline for categorical variables
</span><span class="n">cat_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">'postprocess'</span><span class="p">,</span> <span class="n">clean_categorical</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">'onehot'</span><span class="p">,</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span> <span class="o">=</span> <span class="s">'ignore'</span><span class="p">))</span>
<span class="p">])</span>

<span class="c1"># Pipeline for categorical variables that should be changed into numeric
</span><span class="n">cat_ordinal_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">'postprocess'</span><span class="p">,</span> <span class="n">clean_categorical</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">'impute'</span><span class="p">,</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span> <span class="o">=</span> <span class="s">'most_frequent'</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">'ordinal'</span><span class="p">,</span> <span class="n">change_to_ordinal</span><span class="p">()),</span>
    <span class="p">(</span><span class="s">'scale'</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">())</span>
<span class="p">])</span>

<span class="c1"># Full pipeline that gets applied to data, which combines above three pipelines
# ColumnTransfomer allows different pipelines to be applied to the data by
# passing in the columns it should apply to.
</span><span class="n">full_pipeline</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
    <span class="p">(</span><span class="s">"cat"</span><span class="p">,</span> <span class="n">cat_pipeline</span><span class="p">,</span> <span class="n">cat_columns</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"num"</span><span class="p">,</span> <span class="n">numeric_pipeline</span><span class="p">,</span> <span class="n">num_columns</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"ord"</span><span class="p">,</span> <span class="n">cat_ordinal_pipeline</span><span class="p">,</span> <span class="n">ordinal_columns</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">housing_prepared</span> <span class="o">=</span> <span class="n">full_pipeline</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">training</span><span class="p">)</span></code></pre></figure>

<h3 id="custom-transformers">Custom Transformers</h3>

<p>A useful thing I learned is that you can create custom transfomers and add it to your pipeline. For example, I created a <code class="language-plaintext highlighter-rouge">add_home_area</code> Custom Transformer that adds the total home area to the dataset, which I decided to add for feature engineering. This is used in the numeric pipeline above.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">add_home_area</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="s">"""
    Function to add total home area to numeric variables
    """</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">total_area</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">indexes</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">total_area</span><span class="p">]</span></code></pre></figure>]]></content><author><name>Mia Nakajima</name><email>mb.nakajima@gmail.com</email></author><summary type="html"><![CDATA[I wanted to try getting familiar with sklearn’s machine learning pipeline, so I started applying it to the Kaggle House Prices Competition. The data comprised of housing prices in Ames, Iowa with 79 features. I ended up learning a lot through the whole process.]]></summary></entry></feed>