<!doctype html>
<html lang="en">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
  inlineMath: [['$','$'], ['\\(','\\)']],
  processEscapes: true
}
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management Through Citylearn &middot; Mia Nakajima
    
  </title>

  <link rel="stylesheet" href="/styles.css">
  <link rel="alternate" type="application/atom+xml" title="Mia Nakajima" href="/atom.xml">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management Through Citylearn" />
<meta name="author" content="Kathirgamanathan et al." />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Authors: Anjukan Kathirgamanathan, Kacper Twardowski, Eleni Mangina, Donal Finn" />
<meta property="og:description" content="Authors: Anjukan Kathirgamanathan, Kacper Twardowski, Eleni Mangina, Donal Finn" />
<link rel="canonical" href="http://localhost:4000/minute-papers/01-SACforDSMCityLearn/" />
<meta property="og:url" content="http://localhost:4000/minute-papers/01-SACforDSMCityLearn/" />
<meta property="og:site_name" content="Mia Nakajima" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-15T17:44:50-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management Through Citylearn" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Kathirgamanathan et al."},"dateModified":"2022-09-15T17:44:50-07:00","datePublished":"2022-09-15T17:44:50-07:00","description":"Authors: Anjukan Kathirgamanathan, Kacper Twardowski, Eleni Mangina, Donal Finn","headline":"A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management Through Citylearn","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/minute-papers/01-SACforDSMCityLearn/"},"url":"http://localhost:4000/minute-papers/01-SACforDSMCityLearn/"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body>
    <div class="container content">
      <header class="masthead">
        <h2 class="masthead-title">
          <a href="/" title="Home">Mia Nakajima</a>
          <small></small>
        </h2>
      <nav>
  <ul>
    
      <li><a href="/" >Home</a>
    
      <li><a href="/blog/index.html" >Blog</a>
    
      <li><a href="/projects/index.html" >Projects</a>
    
      <li><a href="/minute-papers/index.html" >One Minute Papers</a>
    
  </ul>
</nav>

      </header>
      <hr>
      <main>
        <article class="page">
  <h1 class="page-title">A Centralised Soft Actor Critic Deep Reinforcement Learning Approach to District Demand Side Management Through Citylearn</h1>
  <p>Authors: Anjukan Kathirgamanathan, Kacper Twardowski, Eleni Mangina, Donal Finn</p>

<p>Link to Paper: <a href="https://arxiv.org/abs/2009.10562">https://arxiv.org/abs/2009.10562</a></p>

<h2 id="1-minute-summary">1 Minute Summary:</h2>

<p>The authors used the soft actor critic deep reinforcement learning approach to reduce and smooth (decrease change from 1 hour to the next) aggregated electric demand of a district with residential and commercial buildings.</p>

<p>The soft actor critic is a reinforcement learning method - it is a model-free off-policy reinforcement learning method that tries to maximize reward by acting as randomly as possible. Off-policy refers to the fact that new policies are not generated solely off of data produced by the prior policy.</p>

<p>They used state space variables like:</p>

<ul>
  <li>Month</li>
  <li>Day</li>
  <li>Hour</li>
  <li>Outdoor temperature</li>
  <li>Direct solar radiation</li>
  <li>Non-shiftable electricity load</li>
  <li>Solar generation</li>
  <li>State of charge of cooling storage - I think this refers to whether they are able to be discharged</li>
  <li>State of charge of DHW storage - I think this refers to whether they are able to be discharged</li>
</ul>

<p>They used a reward function that included penalizing peak consumption and a manual function that rewarded charging at night and discharging during the day.</p>

<p>The authors compared this to a manually tuned rule-based controller (RBC) which charged cooling (and DHW if available) during the night and discharged during the day based on the hour of the day. The authors saw about a ~10% improvement over this baseline using their approach when evaluated over a multi-objective cost function over the peak electricity demand, average daily electricity peak demand, ramping, load factor, and net electricity consumption of the district.</p>

<p>It seems like RL for DSM is an up and coming area and is thought to be promising since buildings have many complex interactive elements that would be hard to model using just a physics-based approach. Also, buildings are usually pretty different from each other and so RL is seen to be able to perhaps better adapt to different groups of buildings than a manually tuned approach like RBC.</p>

<h2 id="interesting-takeaways-or-questions">Interesting Takeaways or Questions:</h2>

<ol>
  <li>What does a 10% improvement help? Who benefits from this?
    <ol>
      <li>From EIA: Demand-side management programs aim to lower electricity demand, <strong>which in turn avoids the cost of building new generators and transmission lines, saves customers money, and lowers pollution from electric generators.</strong> Utilities often implement these programs to comply with state government policies. <a href="https://www.eia.gov/todayinenergy/detail.php?id=38872#:~:text=Demand%2Dside%20management%20programs%20aim,comply%20with%20state%20government%20policies">https://www.eia.gov/todayinenergy/detail.php?id=38872#:~:text=Demand-side management programs aim,comply with state government policies</a>.</li>
    </ol>
  </li>
  <li>The authors mentioned that they would like to find a non-manual reward function so this approach would be more generalizable - I wonder what that would take to make? ðŸ¤”</li>
</ol>

<h3 id="mentioned-papers-interested-in-reading">Mentioned Papers Interested in Reading:</h3>

<p>JosÃ© R. VÃ¡zquez-Canteli and ZoltÃ¡n Nagy. 2019. Reinforcement learning for demand response: A review of algorithms and modeling techniques. Applied Energy 235, November 2018 (2019), 1072â€“1089. <a href="https://doi.org/10.1016/j.apenergy.2018.11.002">https://doi.org/10.1016/j.apenergy.2018.11.002</a></p>

</article>

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2022-09-15T17:44:50-07:00">2022</time>. All rights reserved.
        </small>
      </footer>
    </div>


    
  </body>
</html>
